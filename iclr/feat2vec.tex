\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb}


%%% custom packages by JPG and LA:
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{upgreek}
\usepackage{soul}
\newcommand{\vect}[1]{\vec{#1}}
\DeclareMathOperator{\Feat2Vec}{Feat2Vec}
\DeclareMathOperator{\Word2Vec}{Word2Vec}
\newcommand{\dotp}{\boldsymbol{\cdot} }
\renewcommand{\cite}[1]{\citep{#1}}


\title{Feat2Vec:  Continuous Representations with Semantic Modularity}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}
Methods that calculate dense embeddings for features in unstructured data---such as words in a document---have proven to be very successful for knowledge representation.
Surprisingly, very little work has focused on methods for structured datasets where there is more than one feature type---this is, datasets that have features beyond words.

We study how to estimate  continuous representations for multiple feature types within a dataset,  where each feature type exists in a different higher-dimensional space. 
$\Feat2Vec$ is a novel method that is able  to represent the different feature types in a common space---we call this semantic modularity. 
We demonstrate our work on two datasets.  
The first one is collected from a leading educational technology firm, and thus we are able to discover a common continuous space for entities such as universities, courses, textbooks and students.  
The second dataset is the public IMDB dataset, and we discover embeddings for entities that include actors, movies and producers. 
Our experiments suggest that $\Feat2Vec$ significantly outperforms existing algorithms that do not leverage the structure of the data.
 
\end{abstract}

\section{Preliminaries}
Informally, in machine learning an \textit{embedding}  of a vector   $\vect{x} \in \mathbb{R}^n$  is another vector $\vect{y} \in \mathbb{R}^r$ that has much lower dimensionality ($r \ll n$) than the original representation, and can be used to replace the original vector  in downstream prediction tasks.
%Embeddings are often used to compress sparse categorical variables, and they result into continuous representation of them.
Embeddings  have multiple advantages, as they enable more efficient training~\cite{mikolov2013distributed}, 
and unsupervised learning~\cite{schnabel2015evaluation}.
For example, when applied to text,   semantically similar words are mapped to nearby points.

$\Word2Vec$~\cite{mikolov2013efficient,mikolov2013distributed} is an extremely successful software package that contains two embedding functions with the same domain and codomain:
\begin{align}
\Word2Vec(\vect{x}):&  \{ \vect{x} \in \mathbb{R}^n \mapsto  \vect{y} \in \mathbb{R}^r \}
\intertext{$\Word2Vec$ is suited for calculating embeddings for datasets that consist of documents of words with a vocabulary size of $n$.
Here, $\vect{x}$ is sparse because words (and categorical variables, in general) are  modeled using one-hot encoding.
In this paper we study how to generalize embedding functions for arbitrary datasets with multiple feature types by leveraging their internal structure.
We call this approach  $\Feat2Vec$:}
\Feat2Vec(\vect{x}): & \{ \vect{x} \in \mathcal{F} \mapsto \vect{y} \in \mathbb{R}^r \}\\
  \text{where }  \mathcal{F} =  & \langle \mathbb{R}^{d_1}, \mathbb{R}^{d_2}, \dots, \mathbb{R}^{d_n} \rangle 
\end{align}
Here, $\mathcal{F}$ is a structured feature set---each dimension represents a feature type.
Note that a trivial solution could be implemented by ignoring the structure of the data---in other words, flattening the feature set and simply using $\Word2Vec$.
However, we have the working hypothesis that the structure of the features are useful. 
We design $\Feat2Vec$ such that:
\begin{enumerate}
\item \textbf{Generalize $\pmb{\Word2Vec}$}: When  $\mathcal{F} = \mathbb{R}^n$, $\Feat2Vec$ should be equivalent with a method of $\Word2Vec$.
\item \textbf{Semantic Modularity}. The feature types should independently be projected into a common embedding space.
 $\vect{x}_{\mathcal{F}_i} \in \mathbb{R}^{d_i}$ be the subvector of features defined by the $i$-th dimension of $\upkappa$.
\item Additivity
\end{enumerate}



\section{Feat2Vec}
\begin{itemize}
\item{Model}
\item{Sampling}
\item{Loss Function}
\end{itemize}
\subsection{Structured Deep Factorization Machine}

Feat2Vec relies on Structured Deep-In Factorization Machine, which we describe in detail in a companion paper \cite{deepfm}.
We briefly explain it as an extension to Shallow Factorization Machine~\cite{rendle2010factorization}, which is a binary classifier that scores the likelihood of an observation $\vect{x} \in \mathbb{R}^n$ being labeled  $y \in \{0,1\}$, as proportional to the sum of the factorized pairwise interactions:

\begin{align}
%\hat y(\vect{x}; \vect{b}, \vect{\upphi}, \vect{G})= 
p\bigl(Y=1| \vect{\vphantom{\phi}x}  \bigr ) &= 
\sigma \biggl( 
                       \sum_{i =1}^{n} \sum_{j=i}^{n}  \langle  \vect{\beta}_i ,   \vect{\beta}_j  \rangle \, {x}_i {x}_j
  \biggr)
\end{align}
Here, $\vect{\beta}_i$ is a rank-$r$ vector of factors, and $\sigma$ is the sigmoid activation:
\begin{equation}
\sigma(x) = \log \left(\frac{\exp(x)}{\exp(x+1)} \right)
\end{equation}

Structured Deep-In Factorization Machine enables two improvements:
\begin{itemize}
\item \textbf{Structured features}.
Feature vectors  may have some latent structure that is known beforehand.
For example, consider multiple discrete entity types, like universities, students, and books.
To model this data, one would typically concatenate the one-hot encoding representation of each entity into a single feature vector. 
However, Factorization Machine does not make use of the information that a particular dimension of the feature vector belongs to an entity type.
\item \textbf{Feature Functions}. 
Shallow Factorization Machines are not tractable for large feature sets, because they scale quadratically to the number of features.
Consider complex entities such a natural language, or images. 
Shallow Factorization Machine would need to learn an embedding for each word, and for each pixel.
In many case we may be interested in higher-level embeddings, such as one per passage of text, or one per image.
\end{itemize}

To operationalize this, Structured Deep-In Factorization Machine defines $\vect{\upkappa}$  as a sequence of sequences that define  groups of features. 
Instead of a feature interacting with all other features, they only interact with features from different groups.
For example, consider a model with four features ($n=4$).
If we define 
$
\vect{\upkappa} = \{ \{1, 2 \}, \{ 3 , 4 \} \}
$;
the structure of the model would not allow $x_1$ to interact with $x_2$, or $x_3$ to interact with $x_4$.
Additionally, for each group of features, the model allows applying a $|\upkappa_i| \times r $ feature extraction function $\phi_i$.
More formally, this is:
\begin{align}
%\hat y(\vect{x}; \vect{b}, \vect{\upphi}, \vect{G})= 
p \bigl(Y=1| \vect{\vphantom{\phi}x};  \vect{\phi}, \vect{\vphantom{\phi}\upkappa} \bigr ) &= 
\sigma \biggl( 
                       \sum_{i =1}^{|{\vect{\upkappa}}|} \sum_{j=i}^{|{\vect{\upkappa}}|}  \langle  \phi_i( {x}_{\vect{\upkappa}_i}),  \phi_j( \vect{x}_{\vect{\upkappa}_j}) \rangle
  \biggr)
\end{align}
In this notation, $\vect{x}_{\vect{\upkappa}_i}$ is a subvector that contains all of the features that belong to the group $\upkappa_i$.
Thus, $x_{\vect{\upkappa}_i} =  \{ x_k | i \in \vect{\upkappa}_i\}$. 
The simplest   implementation for $\phi$ is a fully-connected layer (input size  $|g_i|$, and output size $r$).




\subsection{Sampling}



Often, due to the large \# of possible values for a given feature type (i.e. millions of users/movies), it is overwhelmingly costly to feed the model all negative labels, particularly if the model is fairly sparse. A shortcut around this is a concept known as \textit{implicit sampling}, where instead of using all of the possible negative labels, one simply samples a fixed number ($k$) from the set of possible negative labels for each positively labelled record. Typically one samples the feature $w$ that define how the negative labels are constructed (i.e. movies a user didn't watch in the Netflix example) from a flattened multionomial distribution $MN(\{w\},\alpha)$: 
\[p(W=w|\mathbf{x},Y=0)  \propto \hat c_w^\alpha,\alpha \in [0,1] \] 
here \hl{$\hat c_w$}  is the empirical frequency of the $w$ in the dataset, and $\alpha$ is a flattening hyperparameter. This distribution is actually quite general, with $\alpha=1$ corresponding to the empirical frequency distribution, and $\alpha=0$ to a uniform distribution across books.


Our project extends this application by introducing a new implicit sampling method that allows researchers to learn unsupervised embeddings. We can learn the correlation structure within a dataset without any positive or negative labels, simply by generating ``unobserved'' records as our negative sample. I now discuss our method.

We start with a dataset $S$ of records with $p$ features. Features need not be individual numeric columns, but instead can be multiple columns that are sensibly grouped together (say, grouping non-mutually exclusive indicators for comedy,action,drame film as a ``genre'' feature), or even images and text\footnote{this requires usage of \textit{deep-in} Factorization Machines, an extension created by Ralph and Jose previously. It would involve building intermediate layers (e.g. convolutional layers) to map a image/text to a vector}. Often, they are categories, such as user ID, movie ID, book ID, etc. We then mark all observed records as positive, and for each positive record, we generate $k$ negative labels using the following 2-step algorithm:
\\ \\
\begin{figure}
\begin{tcolorbox}
\noindent$\texttt{for }  s \in S \{$
\begin{enumerate}
	\item randomly choose  one of the features to alter for the negative samples from a multinomial over the feature types
	$ f_s \sim \text{MN}(\{|\vec{\phi}|_i\}_{i=1}^p,\alpha_1) $, where $|\vec{\phi}|_i$ denotes the number of parameters associated with feature $i$, representing the feature's relative complexity.
	\item $\texttt{for } (j \in 1,\ldots,k)$
	\begin{itemize}
		\item Assign the negative label an attribute from noise distribution $x_{s,j} \sim \text{MN}(\mathrm{X}_{f_s},\alpha)$, where $\text{MN}(\mathrm{X}_{f_s},\alpha_2)$ is the ``flattened'' empirical multionomial distribution over the frequency of the feature values of feature $f_s$.
	\end{itemize}
	$\}$
\end{enumerate}
$\}$
\end{tcolorbox}
\caption{2-step algorithm for sampling negative label data}
\end{figure}
%we should make note at some point of the impossible interpretability of ``intermediately'' populated features/values because of the wierd hyperbolic shape
%of the probability over alpha (not a mix of alpha=0 to alpha=1). See plot I shared with Jose.
Explained in words, our negative sampling method is to randomly select one of the features, and choose another value for it from a noise distribution $q()$ (here, the empirical flattened multinomial distribution is our noise distribution). Of course, this method will sometimes by chance generate observations with negative labels that \textit{do} in fact exist in our sample of observed records. For this reason, among others, we employ  a technique known as noise contrastive estimation\cite{nce} which uses basic probability laws to adjust the structural statistical model $p(Y=1|\vec{\phi},\vec{x})$ to account for the possibility of random negative labels that appear identical to positively labeled data. An additional burden of this method, however, is we need to learn additional nuisance parameters $Z_{\vec{x}}$ for each unique record type $\vec{x}$ that transform the score function $s(.)$ into a well-behaved probability distribution that integrates to 1. This introduces an astronomical amount of new parameters and greatly increase the complexity of the model. Instead of estimating these, we appeal to the work of \cite{fastnnlang}, who show in the context of language models that setting the $Z_{\vec{x}}=1$ in advance effectively does not change the performance of the model.\footnote{\cite{fastnnlang} suggests one can set the normalizing constant to 1 because the neural net model has enough free parameters that it will effectively just learn the probabilities itself so that it does not over/under predict the probabilities on average (since that will result in penalties on the loss function). Note that while we follow the same procedure, one could set $Z_{\vec{x}}$ to any positive value in advance and the same logic would follow, if one was worried about astronomically low probabilities.}  Written explicitly, the new structural probability model is just:
\[p(Y=1|\vec{\phi},\vec{x}) = \frac{e^{s(\vec{x}_i,\vec{\phi}) })}{e^{s(\vec{x}_i,\vec{\phi}) } + q(\vec{x}_i) }\]

We can show, with relatively little effort,that our 2-step sampler has some interesting theoretical properties. Namely, the embeddings learned under this model will be equivalent to a convex combination of the embeddings learned from $p$ individual Factorization Machines. We show this in a short proof below. let $S_f$ denote the records whose corresponding negative samples resample feature $f$. We can express the loss function $L(.)$, the binary cross-entropy of the data given the \texttt{feat2vec} model, as follows:


\begin{align*}
 L(\vec{\mathbf{x}} | \vec{\phi} ) & =  \frac{1}{|S|} \sum_{i \in S} \Big(\log(\tilde p(Y=1|\vec{\phi},\vec{\mathbf{x_i}}  ) ) + \sum_{\vec{\mathbf{x}}_{j} \sim q(\vec{\mathbf{x_i}})}^k \log(\tilde p(Y=0|\vec{\phi},\vec{\mathbf{x_{j}}})) \Big) \\
 & = \frac{1}{|S|} \sum_{i \in S} \Big( \log(\tilde p(Y=1|\vec{\phi},\vec{\mathbf{x_i}}, i \in S_f )p(i \in S_f) ) + \sum_{\vec{\mathbf{x}}_{j} \sim q(\vec{\mathbf{x_i}})}^k \log(\tilde p(Y=0|\vec{\phi},\vec{\mathbf{x_{j}}},i \in S_f)p(i \in S_f)) \Big) \\
  & = \frac{1}{|S|}\sum_{f=1}^p \sum_{i \in S_f} \Big( \log(\frac{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) } p(i \in S_f)}{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) } + q(\vec{\mathbf{x}}_i | i \in S_f) }) + \sum_{\vec{\mathbf{x}}_{j} \sim q(\vec{\mathbf{x_i}} | i \in S_f)}^k \log(\frac{ q(\vec{\mathbf{x}}_{j} | i \in S_f)p(i \in S_f)}{e^{s(\vec{\mathbf{x}}_{j},\vec{\phi}) } + q(\vec{\mathbf{x}}_{j}| i \in S_f)}) \Big) \\
  & = \frac{1}{|S|}\sum_{f=1}^p \sum_{i \in S_f} \Big( \log(\frac{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) }}{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) } + q(\vec{\mathbf{x}}_i | i \in S_f) }) + \log(p(i \in S_f)^{k+1})\\
  &   +  \sum_{\vec{\mathbf{x}}_{j} \sim q(\vec{\mathbf{x_i}} | i \in S_f)}^k \log(\frac{ q(\vec{\mathbf{x}}_{j} | i \in S_f)}{e^{s(\vec{\mathbf{x}}_{j},\vec{\phi}) } + q(\vec{\mathbf{x}}_{j}| i \in S_f)})   \Big) \\
   & \propto \frac{1}{|S|} \sum_{f=1}^p \sum_{i \in S_f} \Big( \log(\frac{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) }}{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) } + q(x_{i,f}) }) + \sum_{\vec{\mathbf{x}}_{j} \sim q(\vec{\mathbf{x_i}} | i \in S_f)}^k \log(\frac{ q(x_{j,f})}{e^{s(\vec{\mathbf{x}}_{j},\vec{\phi}) } + q(x_{j,f})}) \Big) \\
   & \xrightarrow[|S|\rightarrow \infty]{ }   \sum_{f=1}^p p(i \in S_f) E\Big[ \log(\frac{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) }}{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) } + q(x_{i,f}) }) + \sum_{\vec{\mathbf{x}}_{j} \sim q(\vec{\mathbf{x_i}} | i \in S_f)}^k \log(\frac{ q(x_{j,f})}{e^{s(\vec{\mathbf{x}}_{j},\vec{\phi}) } + q(x_{j,f})}) \Big] \\
   & = \sum_{f=1}^p p(i \in S_f) E\Big[ L(\vec{\mathbf{x}} |\vec{\phi}, \text{target = $f$}) \Big]
\end{align*}
where the third to last line drops the probability of assignment to a feature type term since it is outside of the model parameters $\vec{\phi}$ and fixed in advance.
Thus, the loss function is just a convex combination  of the loss functions of the targeted classifiers for each of the $p$ features, and by extension so is the gradient. Thus it will learn a convex combination of the embeddings for each classifier, with weights proportional to the feature type sampling probabilities in step 1. of the sampling algorithm.

\section{Empirical Results}


\section{Relation to Prior Work}


\section{Conclusion}



\bibliography{joseg}
\bibliographystyle{iclr2018_conference}

\end{document}
