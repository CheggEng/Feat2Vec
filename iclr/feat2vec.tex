\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb}


%%% custom packages by JPG and LA:
\usepackage[most]{tcolorbox}
\usepackage{amsmath}
\usepackage{soul}
\newcommand{\vect}[1]{\vec{#1}}
\newcommand{\dotp}{\boldsymbol{\cdot} }
\renewcommand{\cite}[1]{\citep{#1}}


\title{Feat2Vec: Continuous Representations for Data with Structured Features}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}
Methods that calculate dense embeddings of unstructured data have proven to be very successful for knowledge representation.
Surprisingly, very little work has focused on methods that leverage the internal structure of the data, such as when there are features that  group into different types.

In this paper, we propose Feat2vec as an algorithm that learns a continuous representation for a list of features, where each feature exists in a different higher-dimensional space. 
Feat2Vec is a novel method that is able  to represent the different features in a common space. 
We demonstrate our work on two datasets.  
The first one is collected from a leading educational technology firm, and we discover a common continuous space for universities, courses, textbooks and students.  
The second dataset is the public IMDB dataset, and we discover embeddings for actors, movies and producers. 
Our empirical experiments suggest that Feat2Vec significantly outperforms existing algorithms that discover embeddings from unstructured data.
 
\end{abstract}

\section{Introduction}

\section{Feat2Vec}
\subsection{Structured Deep Factorization Machine}

Feat2Vec relies on Structured Deep-In Factorization Machine, which we describe in detail in a companion paper \cite{deepfm}.
For brevity, we just explain it as an extension to Factorization Machine~\cite{rendle2010factorization}, which is a binary classifier that scores the likelihood of an observation $\vect{x} \in \mathbb{R}^n$ being labeled  $y \in \{0,1\}$, as proportional to the sum of all pairwise interactions:

\begin{align}
%\hat y(\vect{x}; \vect{b}, \vect{\upphi}, \vect{G})= 
p \bigl(\hat{y}=1| \vect{\vphantom{\phi}x}  \bigr ) &= 
\sigma \biggl( 
                       \sum_{i =1}^{n} \sum_{j=i}^{n}  \langle   \vect{x}_i,   \vect{x}_j \rangle
  \biggr)
\end{align}
Here, $\sigma$ is the sigmoid activation:
\begin{equation}
\sigma(x) = \log \left(\frac{\exp(x)}{\exp(x+1)} \right)
\end{equation}

Structured Deep-In Factorization Machine extends this model in two ways:
\begin{itemize}
\item \textbf{Structured features}.
Feature vectors  may have some latent structure that is known before hand.
For example, consider multiple discrete entity types, like universities, students, and books.
To model this data, one would typically concatenate the one-hot encoding representation of each entity into a single feature vector. 
However, Factorization Machine does not make use of the information that a particular dimension of the feature vector belongs to an entity type.
\item \textbf{Automatic feature extraction}. \hl{complete}
\end{itemize}

To address this, Structured Deep-In Factorization Machine defines $G$ as a set of sets that defines groups of features. 
Instead of a feature interacting with all other features, they interact with features from different groups.
For example, consider a model with four features ($n=4$).
If we define G as:
$
G = \{ \{1, 2 \}, \{ 3 , 4 \} \}
$.
The structure would not allow $x_1$ to interact with $x_2$, or $x_3$ to interact with $x_4$.
Additionally, for each group of features, the model would apply a feature extraction function $phi$ that calculates \hl{blah}.
More formally, this is:
\begin{align}
%\hat y(\vect{x}; \vect{b}, \vect{\upphi}, \vect{G})= 
p \bigl(\hat{y}=1| \vect{\vphantom{\phi}x};  \vect{\phi}, \vect{\vphantom{\phi}G}, \bigr ) &= 
\sigma \biggl( 
                       \sum_{i =1}^{|\vect{G}|} \sum_{j=i}^{|{G}|}  \langle  \phi_i( {x}_{G_i}),  \phi_j( \vect{x}_{G_j}) \rangle
  \biggr)
\end{align}
In this notation, $\vect{x}_{G_i}$ is a vector that contains all of the features that belong to the group $G_i$.
Thus, $x_{G_i} =  \{ x_k | i \in G_i\}$. 
Here, $\phi_i$ is a feature extraction feature that inputs the $i$-th feature group of the instance, and returns an embedding.
The simplest implementation is a fully-connected layer (input size  $|\vect{G}_i|$, and output size $r$).




\subsection{Sampling}



Often, due to the large \# of possible values for a given feature type (i.e. millions of users/movies), it is overwhelmingly costly to feed the model all negative labels, particularly if the model is fairly sparse. A shortcut around this is a concept known as \textit{implicit sampling}, where instead of using all of the possible negative labels, one simply samples a fixed number ($k$) from the set of possible negative labels for each positively labelled record. Typically one samples the feature $w$ that define how the negative labels are constructed (i.e. movies a user didn't watch in the Netflix example) from a flattened multionomial distribution $MN(\{w\},\alpha)$: 
\[Pr(W=w|\mathbf{x},Y=0)  \propto \hat p_w^\alpha,\alpha \in [0,1] \] 
here $\hat p_b$  is the empirical frequency of the $w$ in the dataset, and $\alpha$ is a flattening hyperparameter. This distribution is actually quite general, with $\alpha=1$ corresponding to the empirical frequency distribution, and $\alpha=0$ to a uniform distribution across books.


Our project extends this application by introducing a new implicit sampling method that allows researchers to learn unsupervised embeddings. We can learn the correlation structure within a dataset without any positive or negative labels, simply by generating ``unobserved'' records as our negative sample. I now discuss our method.

We start with a dataset $S$ of records with $p$ features. Features need not be individual numeric columns, but instead can be multiple columns that are sensibly grouped together (say, grouping non-mutually exclusive indicators for comedy,action,drame film as a ``genre'' feature), or even images and text\footnote{this requires usage of \textit{deep-in} Factorization Machines, an extension created by Ralph and Jose previously. It would involve building intermediate layers (e.g. convolutional layers) to map a image/text to a vector}. Often, they are categories, such as user ID, movie ID, book ID, etc. We then mark all observed records as positive, and for each positive record, we generate $k$ negative labels using the following 2-step algorithm:
\\ \\
\begin{figure}
\begin{tcolorbox}
\noindent$\texttt{for }  s \in S \{$
\begin{enumerate}
	\item randomly choose  one of the features to alter for the negative samples from a multinomial over the feature types
	$ f_s \sim \text{MN}(\{|\vec{\phi}|_i\}_{i=1}^p,\alpha_1) $, where $|\vec{\phi}|_i$ denotes the number of parameters associated with feature $i$, representing the feature's relative complexity.
	\item $\texttt{for } (j \in 1,\ldots,k)$
	\begin{itemize}
		\item Assign the negative label an attribute from noise distribution $x_{s,j} \sim \text{MN}(\mathrm{X}_{f_s},\alpha)$, where $\text{MN}(\mathrm{X}_{f_s},\alpha_2)$ is the ``flattened'' empirical multionomial distribution over the frequency of the feature values of feature $f_s$.
	\end{itemize}
	$\}$
\end{enumerate}
$\}$
\end{tcolorbox}
\caption{2-step algorithm for sampling negative label data}
\end{figure}
%we should make note at some point of the impossible interpretability of ``intermediately'' populated features/values because of the wierd hyperbolic shape
%of the probability over alpha (not a mix of alpha=0 to alpha=1). See plot I shared with Jose.
Explained in words, our negative sampling method is to randomly select one of the features, and choose another value for it from a noise distribution $q()$ (here, the empirical flattened multinomial distribution is our noise distribution). Of course, this method will sometimes by chance generate observations with negative labels that \textit{do} in fact exist in our sample of observed records. For this reason, among others, we employ  a technique known as noise contrastive estimation\cite{nce} which uses basic probability laws to adjust the structural statistical model $Pr(Y=1|\vec{\phi},\vec{x})$ to account for the possibility of random negative labels that appear identical to positively labeled data. An additional burden of this method, however, is we need to learn additional nuisance parameters $Z_{\vec{x}}$ for each unique record type $\vec{x}$ that transform the score function $s(.)$ into a well-behaved probability distribution that integrates to 1. This introduces an astronomical amount of new parameters and greatly increase the complexity of the model. Instead of estimating these, we appeal to the work of \cite{fastnnlang}, who show in the context of language models that setting the $Z_{\vec{x}}=1$ in advance effectively does not change the performance of the model.\footnote{\cite{fastnnlang} suggests one can set the normalizing constant to 1 because the neural net model has enough free parameters that it will effectively just learn the probabilities itself so that it does not over/under predict the probabilities on average (since that will result in penalties on the loss function). Note that while we follow the same procedure, one could set $Z_{\vec{x}}$ to any positive value in advance and the same logic would follow, if one was worried about astronomically low probabilities.}  Written explicitly, the new structural probability model is just:
\[Pr(Y=1|\vec{\phi},\vec{x}) = \frac{e^{s(\vec{x}_i,\vec{\phi}) })}{e^{s(\vec{x}_i,\vec{\phi}) } + q(\vec{x}_i) }\]

We can show, with relatively little effort,that our 2-step sampler has some interesting theoretical properties. Namely, the embeddings learned under this model will be equivalent to a convex combination of the embeddings learned from $p$ individual Factorization Machines. We show this in a short proof below. let $S_f$ denote the records whose corresponding negative samples resample feature $f$. We can express the loss function $L(.)$, the binary cross-entropy of the data given the \texttt{feat2vec} model, as follows:


\begin{align*}
 L(\vec{\mathbf{x}} | \vec{\phi} ) & =  \frac{1}{|S|} \sum_{i \in S} \Big(\log(\tilde p(Y=1|\vec{\phi},\vec{\mathbf{x_i}}  ) ) + \sum_{\vec{\mathbf{x}}_{j} \sim q(\vec{\mathbf{x_i}})}^k \log(\tilde p(Y=0|\vec{\phi},\vec{\mathbf{x_{j}}})) \Big) \\
 & = \frac{1}{|S|} \sum_{i \in S} \Big( \log(\tilde p(Y=1|\vec{\phi},\vec{\mathbf{x_i}}, i \in S_f )p(i \in S_f) ) + \sum_{\vec{\mathbf{x}}_{j} \sim q(\vec{\mathbf{x_i}})}^k \log(\tilde p(Y=0|\vec{\phi},\vec{\mathbf{x_{j}}},i \in S_f)p(i \in S_f)) \Big) \\
  & = \frac{1}{|S|}\sum_{f=1}^p \sum_{i \in S_f} \Big( \log(\frac{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) } p(i \in S_f)}{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) } + q(\vec{\mathbf{x}}_i | i \in S_f) }) + \sum_{\vec{\mathbf{x}}_{j} \sim q(\vec{\mathbf{x_i}} | i \in S_f)}^k \log(\frac{ q(\vec{\mathbf{x}}_{j} | i \in S_f)p(i \in S_f)}{e^{s(\vec{\mathbf{x}}_{j},\vec{\phi}) } + q(\vec{\mathbf{x}}_{j}| i \in S_f)}) \Big) \\
  & = \frac{1}{|S|}\sum_{f=1}^p \sum_{i \in S_f} \Big( \log(\frac{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) }}{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) } + q(\vec{\mathbf{x}}_i | i \in S_f) }) + \log(p(i \in S_f)^{k+1})\\
  &   +  \sum_{\vec{\mathbf{x}}_{j} \sim q(\vec{\mathbf{x_i}} | i \in S_f)}^k \log(\frac{ q(\vec{\mathbf{x}}_{j} | i \in S_f)}{e^{s(\vec{\mathbf{x}}_{j},\vec{\phi}) } + q(\vec{\mathbf{x}}_{j}| i \in S_f)})   \Big) \\
   & \propto \frac{1}{|S|} \sum_{f=1}^p \sum_{i \in S_f} \Big( \log(\frac{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) }}{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) } + q(x_{i,f}) }) + \sum_{\vec{\mathbf{x}}_{j} \sim q(\vec{\mathbf{x_i}} | i \in S_f)}^k \log(\frac{ q(x_{j,f})}{e^{s(\vec{\mathbf{x}}_{j},\vec{\phi}) } + q(x_{j,f})}) \Big) \\
   & \xrightarrow[|S|\rightarrow \infty]{ }   \sum_{f=1}^p p(i \in S_f) E\Big[ \log(\frac{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) }}{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) } + q(x_{i,f}) }) + \sum_{\vec{\mathbf{x}}_{j} \sim q(\vec{\mathbf{x_i}} | i \in S_f)}^k \log(\frac{ q(x_{j,f})}{e^{s(\vec{\mathbf{x}}_{j},\vec{\phi}) } + q(x_{j,f})}) \Big] \\
   & = \sum_{f=1}^p p(i \in S_f) E\Big[ L(\vec{\mathbf{x}} |\vec{\phi}, \text{target = $f$}) \Big]
\end{align*}
where the third to last line drops the probability of assignment to a feature type term since it is outside of the model parameters $\vec{\phi}$ and fixed in advance.
Thus, the loss function is just a convex combination  of the loss functions of the targeted classifiers for each of the $p$ features, and by extension so is the gradient. Thus it will learn a convex combination of the embeddings for each classifier, with weights proportional to the feature type sampling probabilities in step 1. of the sampling algorithm.

\section{Empirical Results}


\section{Relation to Prior Work}


\section{Conclusion}



\bibliography{joseg}
\bibliographystyle{iclr2018_conference}

\end{document}
