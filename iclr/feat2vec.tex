\documentclass{article} % For LaTeX2e
\usepackage{iclr2018_conference,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{tikz}
%%% custom packages by JPG and LA:
\usepackage[most]{tcolorbox}
\usepackage{amsmath}

\usepackage{upgreek}

\usepackage{soul}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}

\newcommand{\vect}[1]{\vec{#1}}
\DeclareMathOperator{\Feat2Vec}{Feat2Vec}
\DeclareMathOperator{\Word2Vec}{Word2Vec}
\DeclareMathOperator{\q}{{\mathcal{Q}}}
\newcommand{\dotp}{\boldsymbol{\cdot} }
\renewcommand{\cite}[1]{\citep{#1}}
\usepackage{amsthm}
\usepackage{thmtools,thm-restate}
\declaretheorem{theorem}
\title{Feat2Vec:  Semantic Modularity for Continuous Representations of Data with Features}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}


\maketitle

\begin{abstract}
Methods that calculate dense embeddings for features in unstructured data---such as words in a document---have proven to be very successful for knowledge representation.
Surprisingly, very little work has focused on methods for structured datasets where there is more than one feature type---this is, datasets that have arbitrary features beyond words.

We study how to estimate  continuous representations for multiple feature types within a dataset,  where each feature type exists in a different higher-dimensional space.
$\Feat2Vec$ is a novel method that calculates embeddings for data with multiple feature types enforcing \textit{semantic modularity} across features---all different feature types exist in a common space.
We demonstrate our work on two datasets.
The first one is collected from a leading educational technology firm, and thus we are able to discover a common continuous space for entities such as universities, courses, textbooks and students.
The second dataset is the public IMDB dataset, and we discover embeddings for entities that include actors, movies and producers.
Our experiments suggest that $\Feat2Vec$ significantly outperforms existing algorithms that do not leverage the structure of the data.

\end{abstract}

\section{Preliminaries}
Informally, in machine learning an \textit{embedding}  of a vector   $\vect{x} \in \mathbb{R}^n$  is another vector $\vect{y} \in \mathbb{R}^r$ that has much lower dimensionality ($r \ll n$) than the original representation, and can be used to replace the original vector  in downstream prediction tasks.
%Embeddings are often used to compress sparse categorical variables, and they result into continuous representation of them.
Embeddings  have multiple advantages, as they enable more efficient training~\cite{mikolov2013distributed},
and unsupervised learning~\cite{schnabel2015evaluation}.
For example, when applied to text,   semantically similar words are mapped to nearby points.

$\Word2Vec$~\cite{mikolov2013efficient,mikolov2013distributed} is an extremely successful software package that contains two embedding functions with the same domain and codomain:
\begin{align}
\Word2Vec :\, &  \bigl\{ \; \vect{x} \in \mathbb{R}^n \mapsto  \vect{y} \in \mathbb{R}^r \; \bigr \}
\intertext{$\Word2Vec$ is suited for calculating embeddings for datasets that consist of documents of words with a vocabulary size of $n$.
Here, $\vect{x}$ is sparse because words (and categorical variables, in general) are  modeled using one-hot encoding.
In this paper we study how to generalize embedding functions for arbitrary datasets with multiple feature types by leveraging their internal structure.
We call this approach  $\Feat2Vec$:}
\Feat2Vec :\, & \bigl\{\; \vect{x\vphantom{\mathcal{f}}} \in \vect{\mathcal{F}} \mapsto \vect{y\vphantom{\mathcal{F}}} \in \mathbb{R}^r \; \bigr\}\\
\vect{ \mathcal{F} } =\,& \bigl[\; \mathbb{R}^{d_1}, \mathbb{R}^{d_2}, \dots, \mathbb{R}^{d_n} \;\bigr]
\end{align}
Here, $\vect{\mathcal{F}}$ is a structured feature set---where each dimension represents a different feature type.
Notice  that  is possible to use   $\Word2Vec$ on structured data by simply flattening the input.
However,   $\Feat2Vec$  leverage the features' structure  to enforce what we call  \textit{semantic modularity}:
it does not optimize the embeddings of individual subfeatures, instead it reasons at a more abstract level of complexity.
%That is, the embedding of an observation is additive in respect to the embeddings of its feature types;
%thus, each feature type  $\mathcal{F}_i$ must be  projected into the same embedding space.
Semantic modularity is useful because it enables making principled comparisons between different feature types.
%For example, on an educational dataset that contains many feature types, including  students and courses, semantic modularity enables inferring course preferences for students.
%On the other hand, $\Feat2Vec$ is able to generalize on  $\Word2Vec$ when there is only one feature type $\mathcal{F}(n)$.




\section{Feat2Vec}
The rest of this section describes the three parts of a $\Feat2Vec$ implementation.
\S~\ref{sec:model} describes a  model that infers an  embedding from an observation.
To learn this model we need positive and negative examples:
 a positive example is ``semantic'' and it is observed during training, but negative examples are not.
This is similar how a positive example in $\Word2Vec$ are grammatical co-ocurrences, and negative examples are generated.
\S~\ref{sec:sampling} describes our novel sampling strategy.
%This is similar of how $\Word2Vec$ learns the relationships of words  that co-occur.
%Both $\Word2Vec$ and $\Feat2Vec$ resort to sampling implicit samples,  though we propose a different algorithm.
Finally, in \S~\ref{sec:learning} we describe how to learn a $\Feat2Vec$ model from data.


\subsection{Structured Deep-In Factorization Machine}
\label{sec:model}

\citet{levy2014neural} showed that  a $\Word2Vec$ model can be formalized as a Shallow Factorization Machine~\cite{rendle2010factorization} with two features types---a word  and its context.
This factorization model is a binary classifier that scores the likelihood of an observation $\vect{x} \in \mathbb{R}^n$ being labeled  $y \in \{0,1\}$, as proportional to the sum of the factorized pairwise interactions:

\begin{align}
%\hat y(\vect{x}; \vect{b}, \vect{\upphi}, \vect{G})=
p\bigl(Y=1| \vect{\vphantom{\phi}x}  \bigr ) &=
\sigma \biggl(
                       \sum_{i =1}^{n} \sum_{j=i}^{n}  \; \langle \,  \vect{\beta}_i x_i,\,   \vect{\beta}_j x_j  \, \rangle  %{x}_i {x}_j
  \biggr)
\end{align}
Here, $Y$ is the random variable that defines whether the observation is ``semantic'' (which in practice means whether it occurs in the training data),
 $\vect{\beta}_i$ is a rank-$r$ vector of factors, and $\sigma$ is a sigmoid:
\begin{equation}
\sigma(x) = \log \left(\frac{\exp(x)}{\exp(x+1)} \right)
\end{equation}

However, features  may have some structure that is known beforehand.
For example, consider multiple discrete entity types, like universities, students, and books.
To model this data, a Shallow Factorization Machine would  ignore the structure and simply concatenate the one-hot encoding representation of each entity into a single feature vector.
Feat2Vec relies on an novel extension to factorization called  Structured Deep-In Factorization Machine, which we describe in detail in a \hl{companion paper} \cite{deepfm}.
%On the other hand, Structured Deep-In Factorization Machine considers the structure explicitly.
While Shallow Factorization Machine  learns an embedding per feature, the Structured Deep-In model allows greater flexibility.
For example, consider using images or text on these factorization models.
The shallow model learns an embedding for each word, or an embedding per pixel.
The structured model enables higher-level of abstraction and flexibility, and it can learn an embedding per passage of text, or an embedding per image.


Structured Deep-In Factorization Machine inputs $\vect{\upkappa}$  as a vector of vectors that define  the structure of the groups of features.
Each entry $i$  is a vector of size $d_i$ and it is used to represent the feature types of $\mathcal{F}_i$.
%LA: i'm having trouble following this explanation.
In a shallow model, a feature interacts with all other features, but in the structured model they only interact with features from different groups.
For example, consider a model with four features.
If we define
$
\vect{\upkappa} = \bigl[  \, [1, 2 ], [ 3 , 4 ] \, \bigr]
$,
the structure of the model would not allow $x_1$ to interact with $x_2$, or $x_3$ to interact with $x_4$.
Additionally, for each group of features, the model allows applying a $d_i  \times r $ feature extraction function $\phi_i$ that calculates an embedding.
More formally, this is:
\begin{align}
%\hat y(\vect{x}; \vect{b}, \vect{\upphi}, \vect{G})=
p \bigl(Y=1| \vect{\vphantom{\phi}x};  \vect{\phi}, \vect{\upbeta}, \vect{\vphantom{\phi}\upkappa} \bigr ) 
&= \sigma \biggl(
                       \sum_{i =1}^{|{\vect{\upkappa}}|} \sum_{j=i}^{|{\vect{\upkappa}}|}   \bigl\langle \,  \phi_i( \vect{x\vphantom{\beta_i}}_{\vect{\upkappa}_i}, \vect{\beta}_i), \; \phi_j( \vect{x\vphantom{\beta_i}}_{\vect{\upkappa}_j}, \vect{\beta}_j) \, \bigr\rangle
  \biggr) \\
&\triangleq
 \sigma \bigl(s(\vect{x\vphantom{\phi}},\vect{\phi})\bigl) 
\label{eq:deepfm}
\end{align}
For notational convenience, we refer to this model as a scoring function  $s(\cdot)$,
and $\vect{x}_{\vect{\upkappa}_i}$ as the vector that contains all of the subfeatures that belong to the group $\upkappa_i$.
%Thus, $\vect{x}_{\vect{\upkappa}_i} =  \bigl[ \, x_{i_a} | a \in \{1, 2, \dots, {|\upkappa_i|} \} \, \bigr]$.
A simple   implementation for $\phi_i$ is a linear fully-connected layer, where the output of the $r$-th entry is:
\begin{equation}
\label{eq:fully_connected}
\phi_i\bigl(\vect{x\vphantom{\beta_i}}_i, \vect{\beta}_i\bigr)_r = \sum_{a=1}^{d_i}  \beta_{r_a} x_{i_a}
\end{equation}


In $\Word2Vec$, the embeddings for individual words of a document need to be similar.
On the other hand,  $\Feat2Vec$ does not have this constraint. 
Individual subfeatures could even be dropped; as long as feature types from within an observation are similar to each other.



\subsection{Sampling}
\label{sec:sampling}

The training dataset for a $\Feat2Vec$ model consists of only semantic observations.
In natural language, these would be documents written by humans.
Since Structured Deep-In Factorization Machine~(Equation \ref{eq:deepfm}) requires positive and negative examples, we also need to supply  observations that are not semantic.
Consider a  feature type $\mathcal{F}_i \in \mathbb{R}^{d_i}$, that exists in very high dimensional space (i.e, $d_i$ is large).
For example, this could happen because we are modeling with one-hot encoding a categorical variable with   large number of possible values.
In such scenario, it is overwhelmingly costly to feed the model all negative labels, particularly if the model is fairly sparse.

A shortcut around this is a concept known as \textit{implicit sampling}, where instead of using all of the possible negative labels, one simply samples a fixed number ($k$) from the set of possible negative labels for each positively labelled record.
$\Word2Vec$ makes use of an algorithm called Negative Sampling, that has little theoretical guarantees~\cite{samplingnotes}.
In short, their approach samples a negative observation $w$  from a noise distribution $\q_{w2v}$, that is proportional to the empirical frequency of a word $w$ in the training data.

Our  contribution is introducing a new implicit sampling method that enables learning embedding for structured feature sets.
We can learn the correlation of features within a dataset by imputing negative labels, simply by generating unobserved records as our negative sample.
% replaced n <- p;  because we use p for probabilities
Unlike $\Word2Vec$, we do not constraint features types to be words.
Features types can be  individual numeric columns,  but they need not to be.
By grouping subfeatures as   using $\upkappa$ parameter in Equation \ref{eq:deepfm}, the model can reason on more complex entities.
For example, in our experiments on a movie dataset, we use a ``genre'' feature type, where we  group non-mutually exclusive indicators for comedy, action, and drama films.
%For more involved feature types (e.g., an image),  one would need to  define a function $\phi$ that builds intermediate layers  to map the entity to an embedding .

We start with a dataset $S^+ $ of records with $n$ feature types.
We then mark all observed records in the training set as positive examples.
For each positive record, we generate $k$ negative labels using the following 2-step algorithm:

\begin{algorithm}
 \caption{Implicit sampling algorithm for $\Feat2Vec$: $\q$}
\begin{algorithmic}[1]
\Function{Feat2Vec\_Sample}{$S^+, k, \alpha_1, \alpha_2$}
\State $S^-  \leftarrow \emptyset $
\For {$\vect{x}^{\,+} \in S^+ $}
	\State Draw a random feature type $\upkappa_i  \sim \q_1(\{\operatorname{params}(\phi_i)\}_{i=1}^n,\alpha_1) $
	\For {$j \in \{1,\ldots,k\}$}
			\State $\vect{x}^{\,-}  \leftarrow \vect{x}^{\,+} $ \Comment  set initially to be equal to a positive sample
			\State Draw a random subfeature  $\vect{r} \sim \q_2(\mathrm{X}_{ \upkappa_i },\alpha_2)$
			\State $\vect{x}^{\,-}_{\upkappa_i}  \leftarrow \vect{r}$ \Comment{substitute the $i$-th feature type with the sampled one}
			\State  $S^-  \leftarrow S^-  + \{  \vect{x}^- \}$
	\EndFor
\EndFor
\State \Return $S^-$
\EndFunction
\end{algorithmic}
\end{algorithm}


Explained in words, our negative sampling method  iterates over all of the observations of the training dataset.
For each observation $\vect{x}^{\,+}$, it randomly selects the $i$-th feature type from a noise distribution $\q_1(\cdot)$.
Then,  it creates a negative observation that is identical to $\vect{x}^{\,+}$, except that its $i$-th feature type is replaced by a value sampled from a noise distribution $\q_2(\cdot)$.
In our application, we use the same class of noise distributions (flattened multinomial) for both levels of sampling, but this need not necessarily be the case.

We now describe the two noise distributions that we use.
We use  $p_{\q}(x)$ to denote the probability of $x$ under a distribution $\q$.

\textbf{Sampling Feature Types.} The function $\operatorname{params}$ calculates the complexity of a  feature extraction function $\psi_i$.
To sample a feature type, we choose a feature group  from a multinomial distribution with probabilities proportional a feature's complexity.
For categorical variables using a linear fully-connected layer, the complexity is simply the number of categories in the feature type: 
\begin{equation}
P_{\q_1}(\upkappa_i|\{d_i\}_{i=1}^m,\alpha_1)  \
= \frac{d_i^{\alpha_1}}{\sum_{k=1}^n d_k^{\alpha_1}},  \quad \alpha_1 \in [0,1]
\end{equation} 
However, if we have multiple intermediate layers for complex feature types (e.g., convolutional layers), these parameters should also be counted. \hl{why? you should provide some intuition}
The hyper-parameter $\alpha_1$ just helps flatten the distribution. 
 When  $\alpha_1=0$, the feature types are sampled uniformly, 
 and when  $\alpha_1=1$, they are sampled according to their complexity.
Intermediate values are progressions.


\textbf{Sampling Subfeatures.} 
To sample a subfeature  from within a group, we use a similar strategy to $\Word2Vec$ and use the subfeature empirical distribution:
%For  $\q_2(\mathrm{X}_{\upkappa_i}, \alpha_2)$, is the ``flattened'' empirical multionomial distribution over the frequency of the feature values of feature $\upkappa_i$, as in the negative sampling procedure used in $\Word2Vec$ :
\begin{equation}
P_{\q_2}(x_j=x|\mathrm{X}_{\upkappa_i},\alpha_2)  \
= \frac{ \operatorname{count}(x)^{\alpha_2}}{\sum_{x_{\upkappa_i}' \in S^+ } \operatorname{count}(x_{\upkappa_i}')^{\alpha_2}}, \quad \alpha_2 \in [0,1]
\end{equation}
Here, $\operatorname{count}(x)$ is the number of times a subfeature $x$ appeared in the training dataset $S^+$ .
\hl{what happens if the feature is continuous?}

This method will sometimes by chance generate negatively labeled samples that \textit{do} exist in our sample of observed records.
The literature offers two possibilities: 
in the Negative Sampling that $\Word2Vec$  follows, the collusions are simply ignored~\cite{samplingnotes}.
Alternatively, we it is possible to account for o account for the possibility of random negative labels that appear identical to positively labeled data using Noise Contrastive Estimation (NCE)~\cite{nce}.


\subsection{Learning From data}
\label{sec:learning}
We optimize a NCE loss function, to adjust the structural statistical model $p(Y=1|\vect{\phi},\vect{x\vphantom{phi}})$ to account for the possibility of random negative labels that appear identical to positively labeled data.
However, an additional burden of NCE is that we need to calculate a partition function $Z_{\vec{x}}$ for each unique record type $\vec{x}$ that transform the score function $s(\cdot)$ into a well-behaved probability distribution that integrates to 1.
Normally, this would introduce an astronomical amount of computation and greatly increase the complexity of the model.
As a work-around, we appeal to the work of \citet{fastnnlang}, 
who showed that in the context of language models that setting the $Z_{\vec{x}}=1$ in advance effectively does not change the performance of the model.
The intuition is that the underlying model has enough free parameters that it will effectively just learn the probabilities itself.
Thus, it does not over/under predict the probabilities on average (since that will result in penalties on the loss function).
 Note that while we follow the same procedure, one could set $Z_{\vec{x}}$ to any positive value in advance and the same logic would follow, if one was worried about numerical issues with low probabilities.
 
Written explicitly, the new structural probability model is:
\begin{equation}
\tilde p(Y=1|\vec{\phi},\vec{x\vphantom{phi}}) = \frac{\exp\bigl(s(\vec{x\vphantom{phi}},\vec{\phi}) \bigr)}{\exp( s\bigl(\vec{x\vphantom{phi}},\vec{\phi}) \, \bigr) + p_{\q}(\vec{\vphantom{phi}x}| \alpha_1,\alpha_2)}
\end{equation}
where $P_{\q}(.)$ denotes the unconditional probability of a a record $\vec{x_i}$ being drawn from our negative sampling algorithm:
\begin{equation}
\text{\hl{Write equation!! This is only defined in the middle of the appendix}}
\end{equation}

Thus, our loss function $L$ optimizes $\vect{\upbeta}$, the parameters of the function extraction functions $\vect\phi$.
\begin{equation}
L(S)  = \arg\min_\upbeta   \frac{1}{|S^+|} \sum_{\vect{x}^{\,+} \in S^+} \Big(\log(\tilde p(Y=1|\vec{\phi},\vect{x}^{\,+}   ) ) \; + \sum_{\vect{x}^{\,-}  \sim \q(\cdot|\vect{x}^+)}^k \log(\tilde p(Y=0|\vec{\phi},\vect{x}^{\,-}  )) \Big) 
\end{equation}


$\Feat2Vec$ has interesting theoretical properties.
For example, Factorization Machines can be used as a multi-label classifier.
In this case we would have at least two feature groups---one of the feature groups is the label that we want to predict, and the other group(s) is the input from which we want to make the prediction.
The output indicates whether the label is associated with the input ($y=+1$), or not ($y=0$).
$\Feat2Vec$ is equivalent to optimizing a convex combination of the embeddings learned from $n$ individual Factorization Machines.
In other words, it optimizes $n$ multi-label classifiers, by selecting that each feature group is a label for a multi-label classification problem.
See the proof in the Appendix~\ref{thm:proof}.



\section{Empirical Evaluation}


\subsection{Does $\Feat2Vec$ enable better embeddings?}
 Ex ante, it is unclear how to evaluate the performance of an ``unsupervised'' embedding algorithm, but we felt that a reasonable task would be to assess the similarity of trained embeddings using unseen records in a left-out dataset. 
In order to test the relative performance of our learned embeddings, we train our $\Feat2Vec$ algorithm and compare its performance in a targeted ranking task to $\Word2Vec$.
In particular, we evaluate the nearest-neighbors of embeddings of two different entities from which we know the ground-truth.
We evaluate the rankings according to  common  metrics.
\begin{itemize}
\item The first is mean percentile rank (MPR), the average rank (standardized to be on a 0-100 percentile scale):
\[MPR = \frac{1}{N}\sum_{i=1}^N \frac{R_i}{\max R} \]
where $R_i$ is the rank of the entity under our evaluation procedure for observation $i$.
 This measures on average how well we rank actual entities. 
 A score of 0 would indicate perfect performance (i.e. top rank every test sample given), so a lower value is better under this metric. 
\item The next is mean reciprocal rank (MRR), which is similar to mean percentile rank, but weights rankings that are closer to rank 0 (the best rank possible) more:
\begin{align*}
MRR = \frac{1}{N}\sum_{i=1}^N \frac{1}{R_i+1}
\end{align*}
The idea behind this metric is that, under MPR, an improvement  in rankings under an algorithm from  10 to 1 will be equally weighted as an improvement from 49,103 to 49,92 in terms of calculating the $MPR$ statistic. 
Clearly, the former improvement is more useful in a real-world context of submitting reccomendations to users, and so MRR attempts to correct for this by weighting rankings closer to 1 much higher.
A higher value, unlike MPR, indicates better performance. 
\item The final metric we consider is top 1 precision, or the percent of test examples we successfully rank the entity as the top prediction.
\end{itemize}


\subsubsection{IMDB dataset: predicting the director of a film}

The Internet Movie Database (IMDB) is a publicly available dataset\footnote{\href{http://www.imdb.com/interfaces/}{http://www.imdb.com/interfaces/}} of information related to films, television programs and video games.
Though in this paper, we focus only on data on its 465,136 movies.
Table~\ref{tab:features} summarizes the feature types we use.
It contains information on writers, directors, and principal cast members attached to each film, along with  metadata such as length and rating.

\begin{table}[htb]
\centering
\caption{IMDB dataset}
\label{tab:features}
\begin{tabular}{@{}lllp{5cm}@{}}
\toprule
Feature Group Name      & Type              & \# of subfeatures & Example for an instance                                      \\ \midrule
Runtime (minutes)       & Real-valued       & 1                 & 116                                                           \\
IMDB rating (0-10)      & Real-valued       & 1                 & 7.8                                                          \\
\# of IMDB rating votes & Real-valued       & 1                 & 435,368                                                          \\ 
Is adult film?          & Boolean           & 2                 & False                                                        \\
Movie releaes year      & Categorical          & 271               & 2001                                                         \\
Movie title             & Text      & 165,471           & ``Ocean's", ``Eleven"                                  \\
Directors               & Bag of categories & 174,382           & `Steven Soderbergh"                                     \\
Genres                  & Bag of categories & 28                &``Crime", ``Thriller"                                 \\
Writers                 & Bag of categories & 244,241           &``George Johnson'', ``Jack Russell"                      \\
Principal cast members (actors) & Bag of categories & 1,104,280         & ``George Clooney", ``Brad Pitt", ``Julia Roberts" \\\bottomrule
\end{tabular}
\end{table}

We try to replicate the same testing conditions in both algorithms.
Thus each movie in IMDB is encoded as an observation in $\Feat2Vec$, and as a document in $\Word2Vec$ in the most similar possible way.


\subsubsection{Feature Representation}
\paragraph{Word2Vec}
For every movie in the database, we create a document that contains the same information that we feed into $\Feat2Vec$. 
We prepend each feature by its feature group name, and we remove spaces from within features.
In Figure~\ref{fig:ocean11} we show an example document.
Some features may  allow multiple values (e.g., multiple writers, directors).  
To feed these  features into the models, for convenience, we constraint the length by  truncating each feature to no more than10 categories (and sometimes less if reasonable).
We pad the sequences with a ``null'' category whenever necessary to maintain a fixed length.
We do this consistently for both $\Word2Vec$ and $\Feat2Vec$ \hl{Luis: confirm that we pad and truncate both models}.
This results in retaining the full set of information for well over 95\% of our observations. \hl{huh?}

\begin{figure}[ht]
\begin{center}
\includegraphics[width=3.5in]{imgs/w2v_example.pdf}
\caption{Sample document for $\Word2Vec$ for for Ocean 11 movie}
\label{fig:ocean11}
\end{center}
\end{figure}



\paragraph{Feat2Vec} 
Feature representation in $\Feat2Vec$ implies defining feature extraction function for each feature group.
Here, we explain how we build these functions:

\begin{itemize}
\item  \textbf{Bag of categories,  categorical, and boolean:} 
For all of the categorical variables, we learn a unique $r$-dimensional embedding for each entity using a linear fully-connected layer (Equation~\ref{eq:fully_connected}).
This allows  the embeddings of multiple categories per record to be summed.
Thus,  the embedding of ``Brad Pitt`` would be the same when he appears in a movie as a principal cast member, regardless whether he was first  or second star. 
Though, he may have a different embedding when he is listed as a director.
\item \textbf{Text:} 
For the single text feature we use, we  preprocess the text by removing non alpha-numeric characters, pruning stopwords, and stemming the remaining words in the title text.
We then follow the same approach that we did for categorical sequence variables.
It would be straightforward to enable more sophisticated approaches (e.g,  convolutional layers).
\item \textbf{Real-valued:} 
For all real-valued features, we pass these features through a 3-layer feed-forward fully connected neural network that outputs a vector of dimension $r$, which we treat as the feature's embedding.
Each intermediate layer has $r$ units with \texttt{relu} activation functions.
These real-valued features highlight one of the advantages of the $\Feat2Vec$ algorithm: using a numeric value as an input, $\Feat2Vec$ can learn a highly nonlinear relation mapping a real number to our high-dimensional embedding space. In contrast, $\Word2Vec$ would be unable to know ex-ante that an IMDB rating of 5.5 is anything like an IMDB rating of 5.6; it would tokenize both values and treat them as independent words in a document.
\end{itemize}



\subsection{Experimental Setup}
\paragraph{Held-out set} 
For our evaluation, we define a testing set that was not used to tune the parameters of the model. 
To generate this set, we  select randomly 10\% sample of the observations that contain a director that appears at least twice in the database.
We do this to guarantee that the set of directors in the left-out dataset appear during training at least once, so that each respective algorithm can learn something about the characteristics of these directors.
Over 90\% of the movies in the database have exactly one director, if there are multiple, we select the first director in the held-out.
\hl{No dev set?}

 



\paragraph{Hyper-parameters}
For training $\Feat2Vec$, we set $\alpha_1 = 1/4$ and $\alpha_2 = 3/4$.
We perform cross-validation on the loss function to determine the number of epochs to train, and then train the full training dataset with this number of epochs.
While regularization of the embeddings during training is possible, this did not dramatically change results, so we ignore this dimension of hyperparameters.
%The $\Feat2Vec$ algorithm takes approximately 5 minutes per epoch on a home desktop computer.
The $\Feat2Vec$ model is built on the python library Keras
%Cite keras???
After training, we take our left out set of movies, and using the cast members associated with a film, attempt to predict the actual director the film was directed by.
We take the sum of the cast member embeddings, and rank the directors by cosine similarity of their embeddings to the summed cast member vector.
If there is a cast member in the test dataset who did not appear in the training data, we exclude them from the summation.

\paragraph{Results}
For training $\Word2Vec$ embeddings on the IMDB database, we simply treat each movie record as a document, and tokenize all feature values by prepending each feature value in an observation with the name of the variable.
So if a movie has a release year of 1986, we would input the word \texttt{releaseYear\_1986} into that movie's corresponding training document.
We use the \texttt{cbow} $\Word2Vec$ algorithm and set the context window to encompass all other tokens in a document during training, since the text in this application is unordered.
Whenever possible, we set identical hyperparameters for $\Word2Vec$ that we use in $\Feat2Vec$.



Table \ref{tab:eval} presents the results from our evaluation.
As is evident, $\Feat2Vec$ sizably outperforms $\Word2Vec$ along all metrics considered.

\begin{table}[htb]
\centering
\caption{Algorithm Performances on Evaluation Ranking Task \hl{I thought you were also going to include NS?}}
\begin{tabular}{|l|cc|}
\hline
Statistic & $\Feat2Vec$ & $\Word2Vec$ \\ \hline
Mean Percentile Rank & 19.36\% & 24.15\% \\
Mean Reciprocal Rank & 0.0362 & 0.0297 \\
Precision (Top 1) & 2.43\% & 1.26\% \\
\hline
\end{tabular}
\label{tab:eval}
\end{table}

Figure \ref{fig:cdf} shows the full distribution of rankings, rather than summary statistics, in the form of a CDF of all rankings calculated in the test dataset.
The graphic makes it apparent for the vast majority of the ranking space, the rank CDF of $\Feat2Vec$ is to the left of $\Word2Vec$, indicating a greater probability of a lower ranking under $\Feat2Vec$.
This is not, however, the case at the upper tail of ranking space, where it appears $\Word2Vec$ is superior.
One might argue that while, overall, $\Feat2Vec$ appears to perform better, it is exactly that upper tail that actually matters most in practice, and so it would seem $\Word2Vec$ is more useful.
We would argue that actually, intermediate rankings are still strong signals that our $\Feat2Vec$ algorithm is doing a better job of extracting information into embeddings, particularly those entities that appear sparsely in the training data and so are especially difficuly.
But additionally, if we zoom-in on the absolute upper region of rankings (1 to 25), which might be a sensible length of ranks one might give an actual reccomendee, it is the case that up until rank 8 or so, $\Feat2Vec$ outperforms $\Word2Vec$ still, and so even on this metric, is preferred if we only care about top rankings.


\begin{figure}[htb]
\begin{center}
\makebox[0pt]{
\begin{tikzpicture}
\node (img1) at (0,0) {\includegraphics[scale=0.6]{rankcdf.pdf}};
\node (img2) at (1.5,-0.75) {\includegraphics[scale=.3]{rankcdf_top25.pdf}};
\end{tikzpicture}
}
\end{center}
\caption{Cumulative Distribution Function of Director Rankings \\ (With Zoom-in to Top 25 Ranks)}
\label{fig:cdf}
\end{figure}

\subsection{ Educational Dataset}

\hl{Jose to write 2-3 paragraphs only}

\section{Limitations}

\hl{
Continuous features  require a sensible feature function, and future work could focus on them.

Feature engineering (grouping) is important.
}



\section{Relation to Prior Work}
\hl{Jose to write this}

\section{Conclusion}
\hl{Jose to write this}


\pagebreak


{
%\footnotesize
%\setlength{\bibsep}{0pt plus 0.25ex}
\bibliography{joseg}
\bibliographystyle{iclr2018_conference}
}


\appendix
\section{Appendixes}
\subsection{Proof to Theorem \label{thm:proof}}
The embeddings learned with $\Feat2Vec$ are  a convex combination of the embeddings learned from $p$ targeted Factorization Machines for each feature in the data.

\begin{proof}
Let $S^+_f$ denote the positively labeled records whose corresponding negative samples resample feature $f$. We can express the loss function $L(.)$, the binary cross-entropy of the data given the $\Feat2Vec$ model, as follows:

\begin{align*}
 L(\vec{\mathbf{x}} | \vec{\phi} )  =&   \frac{1}{|S^+|} \sum_{i \in S^+} \Big(\log(\tilde p(Y=1|\vec{\phi},\vec{\mathbf{x}}_i  ) ) + \sum_{\vec{\mathbf{x}}_j \sim q(.|\vec{\mathbf{x}}_i)}^k \log(\tilde p(Y=0|\vec{\phi},\vec{\mathbf{x}}_j)) \Big) \\
  =&  \frac{1}{|S^+|} \sum_{i \in S^+} \Big( \log(\tilde p(Y=1|\vec{\phi},\vec{\mathbf{x}}_i, i \in S_f )p(i \in S^+_f) ) + \sum_{\vec{\mathbf{x}}_{j} \sim q(.|\vec{\mathbf{x}}_i)}^k \log(\tilde p(Y=0|\vec{\phi},\vec{\mathbf{x}}_j,i \in S_f)p(i \in S^+_f)) \Big) \\
  =&  \frac{1}{|S^+|}\sum_{f=1}^p \sum_{i \in S^+_f} \Big( \log(\frac{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) } p(i \in S^+_f)}{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) } + P_q(\vec{\mathbf{x}}_i |\vec{\mathbf{x}}_i, i \in S^+_f) }) + \sum_{\vec{\mathbf{x}}_{j} \sim q(.|\vec{\mathbf{x}}_i,i\in S^+_f)}^k \log(\frac{ P_q(\vec{\mathbf{x}}_{j} |\vec{\mathbf{x}}_i, i \in S_f)p(i \in S_f)}{e^{s(\vec{\mathbf{x}}_{j},\vec{\phi}) } + P_q(\vec{\mathbf{x}}_{j}|\vec{\mathbf{x}}_i, i \in S^+_f)}) \Big) \\
  \intertext{Note now that $P_q(\vec{\mathbf{x}}_k|\vec{\mathbf{x}}_i,i\in S^+_f)$ is simply the probability of the record's feature value $\vec{\mathbf{x}}_{k,f}$ under the second step noise distribution $q_2(\mathbf{X_{f}},\alpha_2)$:
   $P_q(\vec{\mathbf{x}}_k|\vec{\mathbf{x}}_i,i\in S^+_f) = P_{q_2}(\vec{\mathbf{x}}_{k,f})$,
   where $k$ refers to record $i$ or a record $j$ negatively sampled from $i$.}\\
=&  \frac{1}{|S^+|}\sum_{f=1}^p \sum_{i \in S^+_f} \Big( \log(\frac{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) } p(i \in S^+_f)}{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) } + P_{q_2}(\vec{\mathbf{x}}_{i,f}) } ) + \sum_{\vec{\mathbf{x}}_{j} \sim q(.|\vec{\mathbf{x}}_i,i\in S_f)}^k \log(\frac{  P_{q_2}(\vec{\mathbf{x}}_{j,f})p(i \in S_f)}{e^{s(\vec{\mathbf{x}}_{j},\vec{\phi}) } + P_{q_2}(\vec{\mathbf{x}}_{j,f})) } \Big) \\
   = & \frac{1}{|S^+|}\sum_{f=1}^p \sum_{i \in S^+_f} \Big( \log(\frac{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) }}{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) } + P_{q_2}(\vec{\mathbf{x}}_{i,f}) }) + \log(p(i \in S^+_f)^{k+1})\\
  &   +  \sum_{\vec{\mathbf{x}}_{j} \sim q(.|\vec{\mathbf{x}}_i,i\in S^+_f)}^k \log(\frac{ P_{q_2}(\vec{\mathbf{x}}_{j,f})}{e^{s(\vec{\mathbf{x}}_{j},\vec{\phi}) } + P_{q_2}(\vec{\mathbf{x}}_{j,f})})   \Big) \\
\intertext{We now drop the term containing the probability of assignment to a feature type $p(i \in S^+_f)$ since it is outside of the learned model parameters $\vec{\phi}$ and fixed in advance:}\\
    \propto & \frac{1}{|S^+|} \sum_{f=1}^p \sum_{i \in S^+_f} \Big( \log(\frac{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) }}{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) } + P_{q_2}(\vec{\mathbf{x}}_{i,f}) }) + \sum_{\vec{\mathbf{x}}_{j} \sim q(.|\vec{\mathbf{x}}_i,i\in S^+_f)}^k \log(\frac{ P_{q_2}(\vec{\mathbf{x}}_{j,f})}{e^{s(\vec{\mathbf{x}}_{j},\vec{\phi}) } + P_{q_2}(\vec{\mathbf{x}}_{j,f})}) \Big) \\
    \xrightarrow[|S^+|\rightarrow \infty]{ }  & \sum_{f=1}^p p(i \in S^+_f) E\Big[ \log(\frac{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) }}{e^{s(\vec{\mathbf{x}}_i,\vec{\phi}) } + P_{q_2}(\vec{\mathbf{x}}_{i,f}) }) + \sum_{\vec{\mathbf{x}}_{j} \sim q(.|\vec{\mathbf{x}}_i,i\in S^+_f)}^k \log(\frac{ P_{q_2}(\vec{\mathbf{x}}_{j,f})}{e^{s(\vec{\mathbf{x}}_{j},\vec{\phi}) } + P_{q_2}(\vec{\mathbf{x}}_{j,f})}) \Big] \\
   & = \sum_{f=1}^p p(i \in S^+_f) E\Big[ L(\vec{\mathbf{x}} |\vec{\phi}, \text{target = $f$}) \Big]
\end{align*}

Thus, the loss function is just a convex combination  of the loss functions of the targeted classifiers for each of the $p$ features, and by extension so is the gradient since:

\[\frac{\partial }{\partial \phi}\sum_{f=1}^p p(i \in S^+_f) E\Big[ L(\vec{\mathbf{x}} |\vec{\phi}, \text{target = $f$}) \Big] =
 \sum_{f=1}^p p(i \in S^+_f) \frac{\partial }{\partial \phi}E\Big[ L(\vec{\mathbf{x}} |\vec{\phi}, \text{target = $f$}) \Big]\]
 Thus the algorithm will, at each step, learn a convex combination of the gradient for a targeted classifier on feature $f$, with weights proportional to the feature type sampling probabilities in step 1 of the sampling algorithm.
\end{proof}


\end{document}
