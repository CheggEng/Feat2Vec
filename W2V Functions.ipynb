{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store W2V Functions in one place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Fit W2V Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import itertools\n",
    "import subprocess\n",
    "import gensim\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from gensim.models.wrappers import fasttext\n",
    "from gensim.models import KeyedVectors\n",
    "from hyperdash import monitor_cell\n",
    "import implicitsampler\n",
    "datadir=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window=71 #max number of columns/words per row\n",
    "embed_dim=30 #number of dimensions\n",
    "output_dir= '' #dir to output model data\n",
    "fasttext_dir  = 'fastText/fasttext' #where fasttext is in the AWS machine\n",
    "fasttext_model = output_dir + 'model' #where we save the model output\n",
    "#name of document lists we save\n",
    "word2vec_train= '{}train_w2v_docs.txt'.format(datadir)\n",
    "word2vec_dev= '{}dev_w2v_docs.txt'.format(datadir)\n",
    "word2vec_test= '{}test_w2v_docs.txt'.format(datadir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "train=None\n",
    "dev=None\n",
    "test=None\n",
    "exclude_tokens = set([v + '_0' for v in ['real_tokens_']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "doclist = [word2vec_train,word2vec_dev,word2vec_test]\n",
    "if generate_wv_data:\n",
    "    for df,i in enumerate([train,test,dev]):\n",
    "        #export the train DF to a datalist\n",
    "        batch_size = 1000000\n",
    "        sentence_fcn = lambda x: ' '.join([w for w in x if w not in exclude_tokens])\n",
    "        with open(doclist[i],'w') as f:\n",
    "            for pos in range(0,len(train),batch_size):\n",
    "                print pos\n",
    "                df_chunk = train.iloc[pos:pos + batch_size].copy()\n",
    "                for c in train.columns:\n",
    "                    tag_fcn = lambda x: c  + \"_\" + str(x)\n",
    "                    df_chunk[c] = df_chunk[c].map(tag_fcn)\n",
    "                if pos==0:\n",
    "                    print df_chunk.head()\n",
    "                df_chunk = df_chunk.values.tolist()\n",
    "                df_chunk = '\\n'.join([sentence_fcn(r) for r in df_chunk])\n",
    "                f.write(df_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define some impt fcns\n",
    "def gen_pairs(doc):\n",
    "    '''\n",
    "    returns all unordered unique pairs in a list\n",
    "    '''\n",
    "    return list(itertools.combinations(doc, 2))\n",
    "def doc_similarity(doc,model):\n",
    "    '''\n",
    "    calculates average cosine similarity between all pairs in a document\n",
    "    '''\n",
    "    pairs = gen_pairs(doc)\n",
    "    return np.mean([model.similarity(p[0],p[1]) if p[0] in model and p[1] in model else 0 for p in pairs])\n",
    "\n",
    "def dev_loss(doc_list,model,num_samples=None):\n",
    "    '''\n",
    "    estimates the loss (-E[similarity]) for a document list\n",
    "    '''\n",
    "    if num_samples is None:\n",
    "        doc_inds = range(len(doc_list))\n",
    "    else:\n",
    "        doc_inds = np.random.choice(range(len(doc_list)),size=num_samples,replace=False)\n",
    "    dev_sims=[]\n",
    "    for iter,ind in enumerate(doc_inds):\n",
    "        if iter%100==0:\n",
    "            sys.stdout.write(\"\\r Evaluated {s}/{ns} Docs\".format(s=iter,ns=len(doc_inds)))\n",
    "            sys.stdout.flush()\n",
    "        dev_sims.append(doc_similarity(doc_list[ind],model))\n",
    "    sys.stdout.write(\"\\n\")\n",
    "    return -np.mean(dev_sims)\n",
    "\n",
    "def run_bash_cmd(cmd):\n",
    "    '''\n",
    "    run a terminal command and feed the output to the terminal in real time\n",
    "    '''\n",
    "    cmd_ls = cmd.split()\n",
    "    p = subprocess.Popen(cmd_ls, stdout=subprocess.PIPE,stderr=subprocess.STDOUT,universal_newlines=True)\n",
    "    for line in iter(p.stdout.readline, \"\"):\n",
    "        sys.stdout.write('\\r'+line.replace('\\n',''))\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#read in development set doc list for evaluation in gensim\n",
    "dev_sentences=[]\n",
    "line_batch_size = 100000\n",
    "with open(word2vec_dev,'r') as f:\n",
    "    while True:\n",
    "        sentence_iter = f.readlines(line_batch_size)\n",
    "        if not sentence_iter:\n",
    "            break\n",
    "        sentence_iter = [s.split() for s in sentence_iter]\n",
    "        dev_sentences+=sentence_iter\n",
    "        sys.stdout.write(\"\\r Read {s}M Sentences\".format(s=len(dev_sentences)/1000000.))\n",
    "        sys.stdout.flush()\n",
    "    print \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define fasttext call\n",
    "cmd = '''\n",
    "{fastdir} cbow -input {datadir}train_w2v_docs.txt \n",
    "-output {model_new}\n",
    "-maxn 0 \n",
    "-minCount 0\n",
    "-dim 30\n",
    "-ws {window} \n",
    "'''.format(fastdir=fasttext_dir,window=window,\n",
    "           datadir=datadir,\n",
    "           model_new=fasttext_model) #the command; note fasttext does alpha=.5 by default\n",
    "\n",
    "cmd = cmd +  '-epoch {}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%monitor_cell fasttext_cv\n",
    "# now train the model by doing line search over optimal # of epochs\n",
    "num_epochs = 10\n",
    "num_dev_obs = 100000\n",
    "np.random.seed(9)\n",
    "dev_subsample  = [dev_sentences[i] for i in np.random.randint(num_dev_obs,size=num_dev_obs)]\n",
    "dev_losses=[]\n",
    "dev_losses.append(0)\n",
    "log = output_dir  + 'epoch_losses.txt'\n",
    "with open(log,'w') as f:\n",
    "    f.write('W2V Performance By Epochs\\nEpoch 0 Similarity: 0 \\n')\n",
    "for e in range(1,num_epochs+1):\n",
    "    print \"Epoch Count: {}\".format(e)\n",
    "    run_bash_cmd(cmd.format(e))\n",
    "    print \"\\nNow Loading Computed Vectors...\"\n",
    "    ft_model = KeyedVectors.load_word2vec_format(fasttext_model+'.vec')\n",
    "    print \"Now Computing Loss...\"\n",
    "    loss = dev_loss(dev_subsample,ft_model)\n",
    "    dev_losses.append(loss)\n",
    "    with open(log,'a') as f:\n",
    "        f.write('Epoch {e} Similarity: {l} \\n'.format(e=e,l=loss))\n",
    "    if loss > dev_losses[len(dev_losses)-2]:\n",
    "        print \"Exiting at Epoch {} due to decrease in Validation Similarity!!!\".format(e)\n",
    "        break\n",
    "    print \"\\n Avg Similarity: {}\".format(loss)\n",
    "    print \"-------------------------\"\n",
    "\n",
    "print dev_losses\n",
    "\n",
    "\n",
    "opt_epoch = dev_losses.index(np.max(dev_losses))\n",
    "#opt_epoch = 10\n",
    "print \"starting optimal {opt_epoch} ...\".format(opt_epoch=opt_epoch)\n",
    "run_bash_cmd(cmd.format(opt_epoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Compare cosine similarity of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "from gensim.models.wrappers import fasttext\n",
    "from gensim.models import KeyedVectors\n",
    "from hyperdash import monitor_cell\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "w2v_model = KeyedVectors.load_word2vec_format('model.vec'.format(experiment_name_w2v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MRR(ranks):\n",
    "    '''\n",
    "    return the mean reciprocal ranks of a list of ranks\n",
    "    '''\n",
    "    return np.mean(1/(np.array(ranks,dtype='float')+1))\n",
    "def avg_vector(doc):\n",
    "    '''\n",
    "    calculates the average of all words in a document fed to the function\n",
    "    often used for calculating an average \"context\" vector\n",
    "    '''\n",
    "    #print doc\n",
    "    vectors = np.array([w2v_model[w] for w in doc if w in w2v_model.vocab])\n",
    "    #print vectors.shape\n",
    "    return np.mean(vectors,axis=0)\n",
    "    \n",
    "def rank_isbns(doc_list):\n",
    "    '''\n",
    "    rank the actual isbn associated with each document across the set of isbn_tokens considered\n",
    "    '''\n",
    "    isbn_list = list(isbn_tokens)\n",
    "    isbn_vecs = np.concatenate([np.expand_dims(w2v_model[i],axis=0) for i in isbn_list],axis=0)\n",
    "    print isbn_vecs.shape\n",
    "    rankings=[]\n",
    "    for i,doc in enumerate(doc_list):\n",
    "        isbn = doc[0]\n",
    "        vec = np.expand_dims(avg_vector(doc[1:(len(doc)-1)]),axis=0)\n",
    "        scores = cosine_similarity(vec,isbn_vecs)[0]\n",
    "        temp = np.argsort(-scores)\n",
    "        ranks = np.empty(len(scores), int)\n",
    "        ranks[temp] = np.arange(len(scores))\n",
    "        rank=ranks[isbn_list.index(isbn)]\n",
    "        sys.stdout.write(\"\\r Ranking: {s}/{l}\".format(s=i,l=len(doc_list)))\n",
    "        sys.stdout.flush()\n",
    "        rankings.append(rank)\n",
    "    return rankings\n",
    "\n",
    "def rank_isbns_byfeature_w2v(doc_list,feature):\n",
    "    '''\n",
    "    Rank ISBNs associated with a given feature.\n",
    "    feature should be the string prefix for all tokens of a given feature type\n",
    "    '''\n",
    "    isbn_list = list(isbn_tokens)\n",
    "    isbn_vecs = np.concatenate([np.expand_dims(w2v_model[i],axis=0) for i in isbn_list],axis=0)\n",
    "    print isbn_vecs.shape\n",
    "    rankings=[]\n",
    "    for i,doc in enumerate(doc_list):\n",
    "        isbn = doc[0]\n",
    "        featureid = (w for w in doc if w.startswith(feature)).next()\n",
    "        if featureid not in w2v_model.vocab:\n",
    "            continue\n",
    "        y = np.zeros(len(isbn_list))\n",
    "        y[isbn_list.index(isbn)] = 1\n",
    "        featurevec = np.expand_dims(w2v_model[featureid],axis=0)\n",
    "        scores = cosine_similarity(featurevec,isbn_vecs)[0]\n",
    "        temp = np.argsort(-scores)\n",
    "        ranks = np.empty(len(scores), int)\n",
    "        ranks[temp] = np.arange(len(scores))\n",
    "        rank=ranks[isbn_list.index(isbn)]\n",
    "        sys.stdout.write(\"\\r Ranking: {s}/{l}\".format(s=i,l=len(doc_list)))\n",
    "        sys.stdout.flush()\n",
    "        rankings.append(rank)\n",
    "    return rankings\n",
    "\n",
    "\n",
    "def rank_isbns_byfeature_fm(df,feature,feature_weight_name):\n",
    "    '''\n",
    "    rank ISBNS associated with a given observation depending on dot product similarity with another feature\n",
    "    feature is the var name\n",
    "    feature_weight_name is the name of the embedding layer the embeddings come from.\n",
    "    '''\n",
    "    isbn_vecs = fm.get_layer('embedding_isbn').get_weights()[0]\n",
    "    feature_vecs = fm.get_layer('embedding_{}'.format(feature_weight_name)).get_weights()[0]\n",
    "    train_isbns = list(isbn_set)\n",
    "    train_isbns.sort()\n",
    "    isbn_train_vecs = isbn_vecs[train_isbns,:]\n",
    "    rankings=[]\n",
    "    for i,r in enumerate(df.iterrows()):\n",
    "        row = r[1]\n",
    "        if feature +'_' + str(row[feature]) not in w2v_model.vocab:\n",
    "            continue\n",
    "        feature_vec = np.expand_dims(feature_vecs[row[feature],:],axis=0)\n",
    "        y = np.zeros(isbn_vecs.shape[0])\n",
    "        y[row['equivalent_isbn13']] = 1\n",
    "        #scores = cosine_similarity(user_vec,isbn_train_vecs)[0]\n",
    "        scores= np.dot(feature_vec,isbn_train_vecs.T).flatten()\n",
    "        temp = np.argsort(-scores)\n",
    "        ranks = np.empty(len(scores), int)\n",
    "        ranks[temp] = np.arange(len(scores))\n",
    "        rank=ranks[y[train_isbns]==1][0]\n",
    "        sys.stdout.write(\"\\r Ranking: {s}/{l}\".format(s=i,l=len(df)))\n",
    "        sys.stdout.flush()\n",
    "        rankings.append(rank)\n",
    "    return rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%monitor_cell w2v_user\n",
    "w2v_user_rankings = rank_isbns_byfeature_w2v(test_docs_4rank,'user_id')\n",
    "fm_user_rankings = rank_isbns_byfeature_fm(test_4rank,'user_id','user')\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(w2v_user_rankings,cumulative=False,label='W2V',normed=1,bins=100,histtype='step')\n",
    "plt.hist(fm_user_rankings,cumulative=False,label='FM',normed=1,bins=100,histtype='step')\n",
    "plt.legend(loc=1)\n",
    "plt.savefig('../model/results/{}/hist_users_comp.pdf'.format(experiment_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: kNN of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sanity check; find nearby ISBNs according to the MFDFM model\n",
    "def knn_isbns_fm(k,isbn_code):\n",
    "    isbn_vecs = fm.get_layer('embedding_isbn').get_weights()[0]\n",
    "    isbn_vec = np.expand_dims(isbn_vecs[isbn_code,:],axis=0)\n",
    "    scores = cosine_similarity(isbn_vec,isbn_vecs)[0]\n",
    "    cur_isbn = scores.tolist().index(np.max(scores))\n",
    "    print cur_isbn\n",
    "    temp = np.argsort(-scores)\n",
    "    ranks = np.empty(len(scores), int)\n",
    "    ranks[temp] = np.arange(len(scores))\n",
    "    ranks=ranks.tolist()\n",
    "    print np.max(scores)\n",
    "    #if title_dict[isbn_code] == '':\n",
    "    #    print \"you should really send an ISBN with a title...\"\n",
    "    #    return None\n",
    "    print \"Most {k} similar books to {title}\".format(k=k,title=title_dict[isbn_code])\n",
    "    cur_rank = 0\n",
    "    num_ranks_shown = 0\n",
    "    print len(ranks)\n",
    "    while num_ranks_shown <= k:\n",
    "        rank_ind = ranks.index(cur_rank)\n",
    "        print cur_rank, isbn_code_dict[rank_ind]\n",
    "        if title_dict[rank_ind]!='':\n",
    "            print cur_rank,title_dict[rank_ind]\n",
    "            num_ranks_shown+=1\n",
    "        cur_rank+=1\n",
    "        \n",
    "\n",
    "def knn_isbns_w2v(k,isbn_code):\n",
    "    isbn_list = list(isbn_tokens)\n",
    "    isbn_vecs = np.concatenate([np.expand_dims(w2v_model[i],axis=0) for i in isbn_list],axis=0)\n",
    "    isbn_vec = np.expand_dims(isbn_vecs[isbn_list.index('equivalent_isbn13_'+str(isbn_code)),:],axis=0)\n",
    "    scores = cosine_similarity(isbn_vec,isbn_vecs)[0]\n",
    "    sorted_scores = np.sort(scores)\n",
    "    print scores\n",
    "    print \"---\"\n",
    "    print sorted_scores\n",
    "    #print scores.shape\n",
    "    cur_isbn = scores.tolist().index(np.max(scores))\n",
    "    print cur_isbn\n",
    "    temp = np.argsort(-scores)\n",
    "    ranks = np.empty(len(scores), int)\n",
    "    ranks[temp] = np.arange(len(scores))\n",
    "    ranks=ranks.tolist()\n",
    "    print \"----\"\n",
    "    #print ranks\n",
    "    print ranks[cur_isbn]\n",
    "    #print ranks\n",
    "    #if title_dict[isbn_code] == '':\n",
    "    #    print \"you should really send an ISBN with a title...\"\n",
    "    #    return None\n",
    "    print \"Most {k} similar books to {title}\".format(k=k,title=title_dict[isbn_code])\n",
    "    cur_rank = 0\n",
    "    num_ranks_shown = 0\n",
    "    while num_ranks_shown <= k:\n",
    "        ranked_isbn_code = int(isbn_list[ranks.index(cur_rank)][len('equivalent_isbn13_'):])\n",
    "        print cur_rank, isbn_code_dict[ranked_isbn_code]\n",
    "        if title_dict[ranked_isbn_code]!='':\n",
    "            print cur_rank,title_dict[ranked_isbn_code]\n",
    "            num_ranks_shown+=1\n",
    "        cur_rank+=1            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create mapper of ISBNs to their titles\n",
    "isbn_codes = pd.concat([train['equivalent_isbn13'].cat.codes,\n",
    "                    dev['equivalent_isbn13'].cat.codes,\n",
    "                    test['equivalent_isbn13'].cat.codes],axis=0,ignore_index=True)\n",
    "isbn_titles = pd.concat([train['book_title'],\n",
    "                    dev['book_title'],\n",
    "                    test['book_title']] ,axis=0,ignore_index=True)\n",
    "title_dict = dict(pd.unique(pd.concat([isbn_codes,isbn_titles],axis=1).values).tolist())\n",
    "isbn_code_dict = dict([(i,j) for j,i in feature_label_dict['equivalent_isbn13'].iteritems()])\n",
    "knn_isbns_w2v(10,feature_label_dict['equivalent_isbn13'][9780201750546])\n",
    "knn_isbns_fm(10,feature_label_dict['equivalent_isbn13'][9780201750546])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
