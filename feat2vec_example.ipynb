{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feat2vec example\n",
    "\n",
    "This script shows a minimum working example for the feat2vec library. feat2vec learns dense vectors for all types of features used in data science- categories, real-valued variables, even images or text. It is an answer to word2vec's widespread usage for vector representation of things that are not really words. It can group \"similar columns\" into single features that have a more sensible interpretation (for example, a 10-dimensional matrix indicating what genres a movie might belong to can be aggregated to a single \"genre\" embedding). We use Factorization Machines and a novel sampling algorithm to generate these data. Please see the PDF in the repo for detailed technical explanation of our learning method.\n",
    "\n",
    "It is built primarily on pandas, numpy, and keras. Below we show a minimum working example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named deepfm",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e089e2e97e4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdeepfm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepfm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeepFM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named deepfm"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os,sys\n",
    "import keras\n",
    "import deepfm\n",
    "import matplotlib.pyplot as plt\n",
    "from deepfm import DeepFM\n",
    "import implicitsampler\n",
    "reload(implicitsampler)\n",
    "from implicitsampler import ImplicitSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some important hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "samplesize = 100000 #number of records\n",
    "batch_size=2000 #number of positive labels per batch\n",
    "neg_samples=5 #number of negative labels per batch\n",
    "embed_dim = 100 #dimensionality of embeddings to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some fake data in a pandas df, including a latent variable that generates a binary variable to create a correlation structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attrs = np.random.multivariate_normal(mean = np.array([0.,0.]), cov = np.array([[1.,.1],[.1,1]]),size=samplesize)\n",
    "data = pd.DataFrame({ 'id': np.random.randint(0,50,size=samplesize),\n",
    "                          'attr1':attrs[:,0],\n",
    "                          'attr2':attrs[:,1],\n",
    "                          'attr3': np.random.uniform(size=samplesize),\n",
    "\t\t\t\t\t\t  'offset_':np.zeros(samplesize)\n",
    "\t})\n",
    "\n",
    "data['item'] = np.round(data.id%10 + np.random.normal(0,.5,size=samplesize))\n",
    "data.loc[data.item >=9,'item'] = 9\n",
    "data.loc[data.item <=0,'item'] = 0\n",
    "\n",
    "data['latenty'] = ( (data.attr3)**4/600. + np.exp(data.attr1*data.attr2) \\\n",
    "                       + (data.item<=3)*np.pi - ((data.item==2) | (data.item==4))*data.attr3 \\\n",
    "                       + ((data.item==8) | (data.item==7) )*data.attr2 \\\n",
    "                       + (data.item>=6)*data.attr1 + (data.id%3)*data.attr3 \\\n",
    "                       + np.random.normal(size=samplesize) - 5 )\n",
    "data.loc[np.abs(data['latenty']) >= 10,'latenty'] = 0\n",
    "data['y'] = (data[\"latenty\"] > 0).astype(int) #  np.floor(data.latenty)\n",
    "dim_y = len(pd.unique(data.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data[\"y\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define what features we are going to use and how we are going to use them\n",
    "Some important things to note here:\n",
    "* Each item in the model_features list will be represented as a dense vector that learns similarities by interacting with all other model features.At the moment, we can pass multiple columns as a single model feature, but this is only available to for real valued features (not categories)\n",
    "* Each item in the sampling_features list represents a unique feature of data that should be sampled together for the oversampling method . For example, one might want to sample car model and car manufacturer together because other combinations are not only not observed in the data, they are entirely nonsensical. sampling_features do not need to coincide with model features (i.e. a researcher might still feel its important to learn separate embeddings for make and model of a car)\n",
    "* features is a list of what we call the model_features in the keras model layers\n",
    "* We pass most arguments to feat2vec as a list of lists, and often each sub-list only has one element. this is necessary because it is possible to have multiple elements in a single \"feature\". \n",
    "* We aggregate attr1,attr2,attr3 into a single \"feature\" called attrs. The idea is these features all represent at a high level one characteristic of each observation, and so should be grouped together. One might imagine in a real world setting, aggregating latitude and longitude into a single \"location\" feature\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['item','id','attrs','y','offset']\n",
    "model_features = [['item'],['id'],['attr1','attr2','attr3'],['y'],['offset_']]\n",
    "sampling_features = [['item'],['id'],['attr1','attr2','attr3'],['y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define how we are going to use the features in the model; specifically, whether to learn biases or embeddings only. typically, we want do not want to learn bias terms since these are not straightforward to import to external applications. Here we only learn embeddings for everything except the intercept term (offset\\_) which we keep to \n",
    "modify the average level of the score function. both bias_only and embeddings_only except an ordered boolean list where positionally they correspond to each feature listed in model_features\n",
    "\n",
    "also pass a realvalued bool list which tells our model whether we should treat the inputs as discrete categories or real-valued scalar numbers\n",
    "\n",
    "finally, feature\\_dims refers to the dimensionality of each feature. for discrete categories, this is the total # of categories. for realvalued features, this is the number of columns (i.e. for attrs it is 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "bias_only = [False]*len(model_features)\n",
    "bias_only[ features.index('offset') ] =True\n",
    "embeddings_only = [True]*len(model_features)\n",
    "embeddings_only[ features.index('offset') ] =False\n",
    "realvalued = [False]*len(model_features)\n",
    "realvalued[ features.index('attrs') ] =True\n",
    "feature_dims = [10,50,3,dim_y,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build the feat2vec model in keras\n",
    "note we specify noise contrastive estimation as the objective; negative sampling is also available but disencouraged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat2vec_obj = DeepFM(feature_dims, embed_dim, obj='nce' ,\n",
    "                      feature_names=features,realval=realvalued)\n",
    "feat2vec_fm = feat2vec_obj.build_model(deep_out=False,bias_only=bias_only,embeddings_only=embeddings_only)\n",
    "print feat2vec_fm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Implicit Samplers that Produce Negative Labels\n",
    "\n",
    "To sample negative labels, we need to choose hyperparameters that determine both how often we sample a given feature, and conditional on sampling a given feature, how often we sample the most popular ones. this parameter is the same as the flattening hyperparameter in word2vec, $Pr(w)$ $\\propto$ $|\\#w|^\\alpha$. Below we show graphically how this parameter changes sampling probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr_item=[]\n",
    "pr_id = []\n",
    "pr_attr=[]\n",
    "pr_y = []\n",
    "stdout = sys.stdout\n",
    "sampled_feature_names = ['item','id','attrs','y']\n",
    "sys.stdout = open(os.devnull, 'w')\n",
    "for a in np.arange(0,1,.01):\n",
    "    tempprobs = implicitsampler.gen_step1_probs(feat2vec_fm,sampled_feature_names,a)\n",
    "    pr_item.append(tempprobs[sampled_feature_names.index('item')])\n",
    "    pr_id.append(tempprobs[sampled_feature_names.index('id')])\n",
    "    pr_attr.append(tempprobs[sampled_feature_names.index('attrs')])\n",
    "    pr_y.append(tempprobs[sampled_feature_names.index('y')])\n",
    "sys.stdout = stdout\n",
    "plt.plot(np.arange(0,1,.01),pr_id,label = 'ID')\n",
    "plt.plot(np.arange(0,1,.01),pr_item,label = 'Item')\n",
    "plt.plot(np.arange(0,1,.01),pr_attr,label = 'Attribute')\n",
    "plt.plot(np.arange(0,1,.01),pr_y,label = 'y')\n",
    "plt.legend()\n",
    "plt.xlabel(r'Flattening Parameter  $\\alpha_1$')\n",
    "plt.ylabel(r'$Pr($Choose Feature$ | \\alpha_1)$')\n",
    "plt.title(r'Feature choice probabilities in step 1 of sampling algorithm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create the probabilities for step1 of our sampler which determines how likely a feature is resampled for producing a negative label. here we choose \n",
    "\n",
    "Pr(choose feature $j$) $\\propto$ $|j|^{.25}$\n",
    "\n",
    "where $|j|$ is the number of parameters associated with  of feature $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_alpha=0.25\n",
    "step1_probs = implicitsampler.gen_step1_probs(feat2vec_fm,sampled_feature_names,feature_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into validation and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pct_train = .90\n",
    "train_sample = np.random.uniform(size=samplesize)\n",
    "train_sample = (train_sample <=np.percentile(train_sample,pct_train*100) ).astype('bool')\n",
    "train_data = data[train_sample==True]\n",
    "valid_data = data[train_sample==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create samplers from implicitsampler class . Here we pass the second main hyperparameter, $\\alpha_2=.25$ (sampling_alpha) to determine how much we flatten our empirical distribution that samples values for negative labels.\n",
    "\n",
    "* sampling_bias adds a minimum count to each unique value in the data to ensure very low frequency values get sampled enough\n",
    "\n",
    "* negative_samples is the number of negative samples per observed record\n",
    "\n",
    "* keep_noise_probs necessary if we use NCE, otherwise should not use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainsampler = ImplicitSampler(train_data,model_features=model_features,sampling_features=sampling_features,\n",
    "                          batch_size=batch_size,                 \n",
    "                          sampling_alpha=.25,sampling_bias=10,\n",
    "                          negative_samples=neg_samples,\n",
    "                          init_probs = step1_probs,keep_noise_probs=feat2vec_obj.obj=='nce')\n",
    "validsampler = ImplicitSampler(valid_data,model_features=model_features,sampling_features=sampling_features,\n",
    "                          batch_size=batch_size,                 \n",
    "                          sampling_alpha=.25,sampling_bias=10,\n",
    "                          negative_samples=neg_samples,\n",
    "                          init_probs = step1_probs,keep_noise_probs=feat2vec_obj.obj=='nce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, fit the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=0)]\n",
    "feat2vec_fm.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=keras.optimizers.TFOptimizer(tf.train.AdamOptimizer()))\n",
    "\n",
    "feat2vec_fm.fit_generator(generator = trainsampler.keras_generator(),\n",
    "                          epochs=100, \n",
    "                          steps_per_epoch = len(train_data)/batch_size,\n",
    "                          validation_data=validsampler.keras_generator(),\n",
    "                          validation_steps =  len(valid_data)/batch_size,\n",
    "                          callbacks=callbacks,\n",
    "                          verbose=1,\n",
    "                          max_queue_size=1,\n",
    "                          workers=1,\n",
    "                          use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage : Look at cosine similarity of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#collect embeddings\n",
    "embeddings = []\n",
    "embed_names = ['dim_'+str(i) for i in range(1,embed_dim+1)]\n",
    "for j in feat2vec_obj.feature_names:\n",
    "    for l in feat2vec_fm.layers:\n",
    "         if l.name=='factor_{}'.format(j) or l.name=='embedding_{}'.format(j):\n",
    "                embedding = feat2vec_fm.get_layer(l.name).get_weights()[0]\n",
    "                embedding = pd.DataFrame(embedding,columns=embed_names)\n",
    "                embedding['value'] = [j +'_' + str(i) for i in embedding.index]\n",
    "                embeddings.append(embedding)\n",
    "embeddings = pd.concat(embeddings,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kNN_embeddings(value,k):\n",
    "    embedding = embeddings.loc[embeddings['value']==value,embed_names]\n",
    "    scores = np.dot(embeddings[embed_names],embedding.T).flatten()\n",
    "    scores = np.array([s/(np.linalg.norm(embedding)*np.linalg.norm(embeddings.loc[i,embed_names])) for i,s in enumerate(scores)]).flatten()\n",
    "    temp = np.argsort(-scores)\n",
    "    ranks = np.empty(len(scores), int)\n",
    "    ranks[temp] = np.arange(len(scores))\n",
    "    for r in range(1,k+1):\n",
    "        print embeddings.loc[ranks==r,'value'].values,scores[ranks==r]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out what the closest embeddings are to ID#5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kNN_embeddings('id_5',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kNN_embeddings('id_5',5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It learned that $mod(10)$ IDs by the same items and have similar y!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
